# Appendix A - Build Your Own Complexity Measurement Tool

### Enumerate Source Code

A system is built from source code. This is any code that a human must write and
maintain. A tool might be written in 100 lines of code to emit an XML file of
one million lines or it could be an algorithm written in 100 lines of C#. Both
of these examples only account for 100 lines of source code, since the million
lines is automatically generated and therefore not source code.

Complexity should be measured to reflect the mental burden associated with the
maintenance of the code. The first dynamic that causes complexity is the sheer
size of the source code. A project of 100,000 lines is more than twice as
complex as a project of 50,000 because complexity grows exponentially with size.

Almost all source code can be organized as files with lines of text. By counting
the files in our project, the lines in each file, and the total lines of source
code in our project, we can generate a basic complexity measure.

For a really cheap measurement, create the following shell script to count C++
code.
    
    find . -name *.cpp - *.h | wc -l

This script will give you an overview of your source code and it is certain
to reveal hot spots that should be addressed. It is based on one key assumption,
that files contain logic and a very long file is an indicator of complexity.
This may be the most sophisticated measure you will ever need.

If you need more metrics, you can build a tool that is based on a deeper
understanding of code complexity. The first task is to find all of our source
code. Here's a tool that lists all the Python files in a directory and reads
the text from each file and removes blank lines.


    def python_source_code():
        return glob(d+'/*.py')


    def read_source(filename):
        text = open(filename).readlines()
        lines = [x for x in text if x.replace(' ','')]
        return lines


    def list_code():
        for f in python_source_code():
            for line in read_source(f):
                print(line)

    list_code()


### Measure Module Size

You can easily adapt this code to your environment and language choices.
Building your own tool can be far better than using a pre-built tool because it
reflects your own specialized and dynamic needs. The next step in building a
complexity tool is to measure the size of each of the files in the source code.

    def module_size():
       for f in python_source_code():
            source = read_source(f)
            print(f + ' lines:' + source)


### Calculate Complexity

Now we can account for the exponential impact of size on complexity by
converting the print function to a complexity term. An exponent of 1.2 is used
to account for the interactions of each line with the other lines in the file.
This is a parameter that you can tune to reflect your situation. The value could
be set from 1.0 (reflecting no internal interaction) all the way to 2.0
(reflecting that every line affects every other line). A good starting point is
1.2 until you have more data.


   def module_complexity():
       for f in python_source_code():
            source = read_source(f)
            complexity = len(source) ** 1.2
            print(f + ' complexity:' + complexity)


The next development is to look at the functions within the module. We will
identify the length of each function within the source code. The is_function
code locates the functions within the source and the list_functions logic
calculates the lines within the function.

    def is_function(line):
        if line.strip().startswith('def'):
            pat = compile(r"\s*def (.*)\s*\(.*")
            name = pat.sub(r'\1',line)
            return name


    def list_functions(lines):
        start = 0
        for i,line in enumerate(lines):
            function_name = is_function(line)
            if function_name:
                name = function_name
                print(name, i-start)
                start = i
        print(name, i-start)


### Estimate Non-linear Complexity

The next step is to assess the relative cost of each of the functions. This time
we use a 1.3 exponent to reflect the deeper interconnection of the lines of code
within the function. This setting could reasonably take on any value in the
range of 1.1 to 2.0.


    def function_cost(name,size):
        cost = size ** 1.3
        print('    %-26s %8d %8d\n' % (name, size, cost))


Now that you have seen the individual pieces we will integrate the whole thing
together into a program. Our tool will do the calculations and build the total
complexity measurement in memory, then print the summary.


    def count_functions(lines):
        results = []
        start = 0
        for i,line in enumerate(lines):
            function_name = is_function(line)
            if function_name:
                name = function_name
                results.append(name, i-start)
                start = i
        results.append(name, i-start)


    def list_modules():
        return [ f,read_source(f) for f in python_source_code() ]



Note, count_functions measures the size of each function and returns the data as a
Python list. The function list_modules returns a list of all the source code
text. The calculate_complexity function now goes through this list data and
converts it into a complexity metric.


### Estimate Module Complexity

Now let's look at calculating the cost of each module within our system. The
cost of each function within the module is summed. Then an exponential factor is
applied to the total. This compensates for the extra penalty of many functions
within the module.

    def module_cost(lines):
        module_cost = 0
        summary = '\n'
        name = 'module' 
        for x in count_functions(lines):      
            cost,details = function_cost(name, x[0])
            summary += details
            module_cost += cost
            name = x[1]
        return module_cost,summary
        

    def cost_of_modularity(lines):
        size = len(lines)
        return (size/2) ** 1.1


    def complexity(filename, lines):
        num_lines = len(lines)
        cost, summary = module_cost(lines)
        cost += cost_of_modularity(lines)
        s='%-30s %8d %8d %s'%(filename, num_lines, cost, summary)
        print(s)
        return (num_lines, cost, summary)


    def show_complexity():
        print('File                        Lines  Complexity\n')
        total_cost = 0
        total_lines = 0
        for filename,source in list_modules():
            num_lines, cost, summary = complexity(filename, source)
            total_lines += num_lines
            total_cost += cost
        print('\n%-30s %8d %8d' % ('total',total_lines,total_cost))


You now have your first complexity metric and can go all the way from source
code to a summary complexity map of your code. You can control every assumption
and code in additional attributes, as needed.
Analyze your code and correlate your knowledge of the coding effort with the
complexity metric from the tool. As you gain insight, make adjustments to the
tool to reflect a deeper understanding. For example, many imports add system
complexity so build an import detector and a penalty term for each import. 
