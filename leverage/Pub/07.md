# 7 - Test Leverage


> If you don't like testing your product, most likely your customers won't 
> like to test it either.
>
> ~ _Anonymous_

---

## Testing - Traditional or Practical

There appears to be a gap between the traditional ideas of testing and quality
assurance and the very specific methods and techniques taught by the advocates
of Test-Driven Development. The classic discussions focus on planning and
building large documents to capture the Test Plan, Risk Analysis, Test
Strategy, Test Tools, Test Cases and Defect Tracking. The primary artifacts
generated are documents and the core testing activities aren't automated.
Large numbers of people are hired to interact with the system.

The other main school of thought is based in the principles of Agile Software
Development. The key ideas here deal with automated testing and unit test
frameworks. These are used throughout the implementation phase and serve to
ensure the quality long after the implementation is complete. I believe that
these concepts are the foundation that every testing effort should build on.

Test-Driven Development, in the form most often taught, assumes that you begin
using the correct testing approach at the beginning of the project. Tests are
built as the product is built. The two parallel infrastructures validate each
other by confirming all of the embedded assumptions. Development with two
parallel structures that are mutually reliant on one another is difficult to
achieve after the fact.

What if you are given a million lines of legacy code? Is it really practical
to tell your stakeholders that your first job is to produce a million lines of
test code so that you can do reliable refactoring? Of course not! We need a
method to apply the principles of TDD to existing systems.

This chapter attempts to fill the gap between the ideals of TDD and the
practical realities of legacy software. We will remain true to the ideals of
rigorous testing while making it easy to add and maintain tests that utilize
existing parts of the system. In order to achieve the maximum leverage, we
need test strategies that can be used effectively for both greenfield
applications and legacy code.


## Expected Results

Traditional testing techniques have many problems that end up compromising the
overall test effort. In the previous chapter we introduced a new style of
testing that works well for test-driven development. We will continue to build
upon these techniques to show a broad range of testing strategies, tactics,
and tools.

The techniques taught here are useful over a broad range of applications and
software types. We will demonstrate how to very quickly create and repair
tests and assemble them into a huge inventory of test cases. Small test shims
can also be added to any type of program to exercise the built-in product
functionality for the purpose of testing. We will examine how to construct
test cases for different types of situations. Finally, we will wrap up with a
look at building an effective testing strategy.


### Every Test has Output

Our idea of testing is based on every test passing out some type of output.
Some tests will pass out text that represents errors that occur. These tests
are typically silent if everything works correctly. Other tests may pass out
lots of data that is the same each time the test is run.

When a test is run for the first time the output is captured for later. When
the test is rerun, the same output is expected. Each time the test is run the
actual results are compared to the expected output. This ends up creating a
very simple contract for each test.

* run the script
* capture the output
* should be the expected results


### Unexpected Results

The Unix diff command is used to detect the unexpected results. These
differences are then shown to the tester as a test failure. The assumption is
that the test should produce the same output each time it is run. If not,
there is a mystery to investigate.

In real life the output of many tests contains results that vary each time the
command is run. This means that without additional work put into the test, it
will fail every time. In practice each test starts off noisy and then it is
modified by filtering the output to quiet it down. For example, consider
the following test 'ls -l', which  produces the following output.

    total 72
    drwxr-xr-x@  26 markseaman  staff    884 Apr  6 15:48 Code
    drwx--@  19 markseaman  staff    646 Jul 11 09:18 Dropbox
    drwxr-xr-x@   3 markseaman  staff    102 Feb 21 15:16 Money
    drwxr-xr-x@   9 markseaman  staff    306 Apr  7 11:33 MyBook
    drwxr-xr-x@   2 markseaman  staff     68 Apr 10 07:27 Notebook
    drwxrwxr-x    5 markseaman  staff    170 Jun  7 09:00 logs
    drwxr-xr-x  153 markseaman  staff   5202 Jun 23 08:45 test

This works perfectly if you want to watch any files that change, but it will
fail repeatedly if the files are changing and you don't care. In that case you
would switch to 'ls' to ignore the things you don't care about. Here are
several ways to silence noisy tests:

* Ignore all output - This will mute the test except for errors.
* Look for special patterns only - Mute everything else.
* Remove problematic text from output - Use "grep -v " to remove stuff.
* Count lines only - Look only at size of the output (min, max).
* App logic that knows the content of the output - Pattern analysis.

In general, you should try to use the simplest thing that will work. To be
effective at Diff Testing you need to be able to resolve a test problem in
less than a minute. The filtering techniques listed above are in the order
that yields the simplest solution first.


### Approve Results

Any test that has unexpected results for its output is considered as a test
failure. But these results don't always correspond to an actual system
problem. They simply inform you that there is a mystery. Frequently you can
look at the unexpected result and immediately understand why it was reported.
This can be approved so that next time it is the required answer.

The system is deliberately constructed to bring your attention to things that
are changing. Often this is a confirmation that the last action you took had
the desired effect. Once you recognize what the test is telling you it can be
approved so that this answer will be expected on all subsequent test runs.

For example, all of the text files used to write this book are processed with
the following command ('wc -w *') to count the number of words. Adding a new
chapter or editing the chapter text will make this test pop. The result can be
approved with the command, 'tst like book-words'. Now the result that I just
got is the new required answer. This makes accepting new results a trivial
operation.

In real life, you build hundreds of tests which are run all of the time.
Almost everything that you do changes something somewhere somehow. Your tests
find these things and bring them to your attention. Then you look at all of
the results and approve them one at a time - you can see quickly what failed
and why. If many tests fail and you scan the results, you may want to approve
everything in one shot with:

    tst accept

This is exactly the same as visiting each test and approving it. This gives
you control over what is approved without having to do the tedious work of
approving each test result individually.


### Dealing with Failure

Not every output that comes from a test is correct. Sometimes the unexpected
result is a symptom of a real system failure. The value of our approach is
that you can watch a thousand things and the system tells you to pay attention
to the ten unexpected results. Diff tests focus my attention to the areas
that may have problems.

A test failure may be resolved in several ways:

* The failure is intermittent and goes away with the next test
* The test is noisy and its output should be filtered more
* The test is bad and is reporting a false positive
* The unexpected result should be approved
* The product is broken and should be fixed

The nature of the output makes it very easy to troubleshoot failing tests. It
is usually possible to fix ten failing tests in less than one minute. In fact,
many times you can spin through the results (using 'tst results') and then
approve all results with 'tst accept'.

The real failures are fixed by either improving the product code or the test
code depending on which is really in error. In practice about half of the
errors are in the product code. This is consistent with the idea that half of
all the code is product code.


## Speed Goals

The most important goal of a test system is that it is easy to create new
tests, run all tests, and fix failing tests. If these tasks are difficult,
testing will be abandoned. Trying to implement TDD in a project and not
succeeding is far worse then never trying at all.

I have been practicing unit testing for 25 years. The most common reaction I
get when trying to get an organization to use the techniques, is "Oh, we tried
testing once and it didn't work for us". As a result, these organization have
abandoned testing altogether in favor of lots of people using the product and
logging defects. Another recurring fallacy is the belief that developers can
do their own testing. Most engineers tend to be poor testers unless you can
redefine testing to be a development task.

The reason that most attempts at integrating automatic testing into the
software development process fails is that it is too hard to maintain the
tests. If it were easy to create, run, and approve tests, everyone would do
it. The techniques proposed here will let you do just that.

In 15 seconds you should be able to ...

* execute all tests
* create a new test
* approve the answer you just got for a test
* modify the test case

If any of these tasks take longer than a minute then you are doing something
wrong. Speed and ease are far more important than closing one small loop hole.
Your goal should be to create a thousand simple tests over time. Each test
should be roughly one line of code. If it is that easy then tests will create
themselves. Testing will become an automatic task and engineers won't even be
aware that they are doing it.


### Time to Create a Test

The first performance goal is the time that it takes to create a simple test.
A command lets you type a single line of code to create a new test with a
simple function. Here are a couple of examples:

    tst add list-files 'List the files in my directory' 'ls'

    tst add code-length 'Count lines in code' 'wc -l xxx.cpp'

You should be able to quickly perform any of the core actions within 15
seconds. In an hour you should be able to generate close to 100 tests for a
system that you already know quite well. This is perfect for when you are
trying to implement numerous tests for a legacy system. A competent engineer
should be able to wrap an entire system in tests in about a day.

Now, let's look at how all this works. Tests are just shell scripts that
produce output. We could get fancier than that but we don't need to now. When
a test is executed we capture the output including any errors that were
produced.


### Time to Run Tests

Now these new commands are part of our arsenal and can be run with the 'tst'
command. The status can be reviewed with the 'tst results' command and all
tests will be run automatically. Every test must run without requiring any
input to be provided. A complete test battery should run in less than 10
seconds.

There are two strategies for preventing a delay for tests that require a long
time to run. The first is to group all of the long running tests into a
separate test suite. While doing development, the fast tests are run
frequently and the slow tests can be run automatically on a daily or even
weekly basis to test complex system operations. When we discuss systems
operations maintenance we will revisit this idea.

The second strategy is to cache long running test results. Diff tests have a
built-in caching mechanism that does this for me. The execution of each test
is timed and the result is cached for 100 times as long as the execution time
of the test itself. When a new test execution is requested, the cached value is
returned and the test delay is avoided.

After a period of time the cache will expire and the results are discarded.
This forces the execution of the test on the next request. The command 'tst
forget' will force all tests to execute. This technique helps the runtime of
the typical case but still has some exposure in the extreme.


### Time to Fix a Failure

Each time a test is executed, we compare the output of the test with what we
were expecting. The differences between the actual and expected results are
shown - they are calculated by using a 'diff' script. If the actual results
match expected results, then diff generates no output and it is a passing
test.

One way to fix a failing test is to approve the actual results returned from
executing the test. This makes the current output the right answer for the
next time. Here are a couple of examples:

    tst like list-files 

    tst like code-length 

If the test needs to be edited, the following commands might be used. This
automatically runs the editor on the correct file.

    tst edit list-files

    tst edit code-length

It should be very easy to make small changes to any test in a few seconds. A lot
of functionality can be added to these commands to solve other problems, but
this is a perfectly useful starting point for your testing activities. 


### Frequency of Execution

Given how easy it is to execute all of the tests and act on the results, you
should be able to execute all of the tests at least every five minutes. When I
am working in heavy incremental development the tests are executing every 
minute.

This testing style is also ideal for executing as a part of a continuous
integration tool. Every code commit can kick off an integration text. Why not
refuse to accept the code until the tests pass clean? This will prevent
developer mistakes related to committing bad code.

Make your tests painless to run. If they are fast they will be run without
requiring conscious thought or deliberate intention. Our goal with this
strategy is to create a habit that becomes so ingrained that any alternative
is unthinkable.


## Test Cases

How can we create robust individual test cases that are easy to maintain over
time? Our testing code will ultimately be composed of hundreds of individual
test cases. Each test case should be incredibly simple and limited to a couple
of lines of code.

Now we will turn our attention to some practical advice on how to build test
cases for our system. We will examine how to structure a test so that we can
easily filter the output. Then we will explore a couple of interesting
applications for using our Diff Testing system. Common testing scenarios are
needed to support testing with live data and testing web servers - these both
require some special attention.


### Initial Construction

There are two levels of structure in the diff test implementation. The highest
level of design deals with Test Suites. These provide for the testing of an
entire subsystem or a specific type of data. The test suite acts as a
container for test cases. The suite can be executed by itself to perform all
tests related to a specific type of data. For example, 'brain test' would
execute all of the tests available for the brain data type.

The second level is where we find the test cases. Each test case is a function
that generates some output and can be approved independently. The suite simply
enumerates the available test cases and provides a context for execution.

Here's how to construct test cases that provide you with the maximum leverage
opportunity. Keep your test cases simple. In most cases you already have code
in your product that does the thing you wish to test. Simply invoke the
function with a single call and print the returned result. For example,
'print(list_customers())' is a nicely designed test case - no extra fluff
here.

Use this as a launch point for creating new test cases. Simple is always the
best first step. The assertions that are common to other forms of testing are
provided by the output checker. A test case is guaranteed to produce the
expected result or it will be reported. This means that you can skip the step
of identifying the correct output by simply approving the results that you are
happy with and fix the others.


### Filtering the Output

Some output is not important to the success of a test. This output can be
filtered so that it is ignored before the results are checked. This will
remove certain kinds of variations that aren't of interest. Some actions will
produce unexpected output every time they are run. There are several design
patterns that assist you in producing tests quickly that strike the right
balance between rigor and maintenance burden.

The text that is captured from the output of the test can be run through a
filter that removes the text that varies from run to run. There are a number
of different filters that help you handle specific situations. Each of these
filters removes incidental changes from the output or transforms the output in
a way that you can use as expected results.

* **no_output** - discard all output so that this test can never fail
* **text_filter** - remove text string of a pattern from the output
* **line_filter** - remove lines that contain the pattern text from output
* **line_limit** - set a range of lines that the output must have
* **path_filter** - eliminate file paths for the test directory
* **time_filter** - remove time and date info
* **replace_filter** - convert text pattern to string
* **match_filter** - save only the lines that match the pattern

Each of these functions lets you post-process your test output so that you can
quickly construct the test that you really want. Here are some examples of how
this might be used in real life:

    def customer_test():
        line_limit(list_customers(), 10, 15)

    def spin_up_servers_test():
        no_output(spin_up_servers())

    def list_files_test():
        time_filter(system('ls -l'))

    def list_users_test():
        line_limit(match_filter(list_users(), 'seaman'), 2, 4)


This approach is clean, simple, and produces code that is quite readable. It
reveals the true essence of what is being tested. It allows you to ignore the
problematic output while preserving the rest. Imagine the expressive power
that is now at your disposal!


### Testing with Data

The most difficult problems encountered during testing involve how to
manipulate saved state. I believe that this explains the rising interest in
functional programming. If a block of logic is stateless then it is extremely
easy to test. Every time you use a stateless function you supply all of the
data as arguments.

But in the real world most of our systems are extremely "stateful" so we need
to develop patterns for testing the state in our systems. One common pattern
is the setup/teardown pattern. Each test run begins with a setup function that
creates all of the required state. In some cases, such as the Django unit test
framework, entire databases are created from scratch. The initial state is
then transformed by the execution of all of the test code. Because the system
starts with a known state the code can verify the validity of a new state.
After all of the testing is completed the system can be restored to a good
state by the clean up function.

To effectively use this pattern you must be very clear about the level of
state change that occurs during set up and clean up. You also must figure out
if new data is inserted into the system incrementally or if the entire state
is reloaded from storage. For example, if the database is reloaded from a
file the operation will destroy any state modifications from the previous
data.

Tests can also create side effects in the system state. Tests may be coded to
algorithmically insert and delete data. This requires that more careful
thought be given to the side effects of the tests but it also opens the
possibility of using the existing state as a starting point.

Choose the techniques that work best for the unique situations that you are
facing. Build standard solutions to common problems and get everyone on your
team to use the best practices. Your overall testing should include everything
from heavy destructive testing to subtle and incremental checks.

Test fixtures allow the system to advance instantly to a known state. Fixtures
are often encoded as JSON data and loaded into the system. Instead of
replaying the transactions in a story, you can jump to the end result. This
lets you set the starting state on the system, add some new transactions, and
then check the ending state.

For example, "save game MyGame” and "load game MyGame” could allow us to save
the entire game and recall it from a file. Now entire scenarios can be tested
with live data on the live system. This pattern can be encapsulated so that it
becomes easy to load state, run tests, confirm state, and restore state.
Engineers can then focus on creating the scenarios that need tested.

In a real system you need a robust server strategy to safeguard against
corruption of production data. Several tiers of servers is essential:

* Production - Enable passive monitoring and notifications of errors but never
allow live data testing 
* Staging - Identical to production except for the fundamental connections to 
customers
* Test - All servers used to test the software
* Dev - Servers used to by engineers to develop and test the software 

Make sure that everyone in your organization understands and follows the
guidelines for each server environment. I recommend that you put automation in
place to enforce the correct behavior rather than relying on the discipline of
the individuals.


### Testing Business Logic

There is a huge variety of tests that you'll need to construct throughout the
course of a project. The design goal of diff testing is to give you a common
set of tools that you can configure to meet the specific needs of any
particular situation.

There are some common situations that you will likely encounter as you begin
to dive into this new style of testing. Develop a catalog of design patterns
that represent the solutions that you face in your unique business situation.

Identify the key interfaces throughout your system. Learn the language of each
API. Implement a single test case for each operation in the API. Create a test
suite for the API and include all of the test cases related to the API. This
makes for a simple but powerful way to organize all of your testing.

Identify all of the key data that is either imported on exported from your
system. If you import and export an employees data type from your system there
should be a test that looks like:

    def import_export_test():
        print(import_employees('bogus_employees.txt'))
        print(export_employees())

This will let you keep an eye on the import/export operation without conscious
effort on your part. If Wally accidentally breaks the test case three months
from now you will know.


### Web Page Testing and Browser Automation

Web development is an important part of most software these days. You need an
easy way to confirm that your web servers are working correctly and that the
application logic is also operating correctly. This testing can be done either
by accessing the web services directly or by remote control of a browser.

Curl is a simple tool that will allow you to make http requests directly to a
web server. This bypasses all of the interactions that may be performed within
the browser. There are options for things like logging into the web site and
passing input to pages but these constructs are fairly primitive and I
recommend against building a lot of testing logic using Curl. However, as a
page fetch engine it will let you quickly create simple tests.

Selenium is another useful tool that acts as a remote controller for a web
browser. It works on top of all of the popular browsers. You can use it to
fetch certain pages from the web server and find certain elements in the DOM.
You can even script activities like logging in and entering text in a search
box.

Selenium is the best tool for testing web servers through a local browser.
Because Selenium runs on the client, no special software needs to be installed
on the server. This means that tests can be run from anywhere. A simple test
can monitor and ensure that the production server is up and running.

Once the infrastructure is in place it is easy to keep expanding this testing.
Scripts can contain logic to detect a wide range of errors that might occur in
your application. Your only limitation is your imagination and the time you
are willing to invest in contriving test scenarios.

Pages can be fetched from the server so you can easily compare either the text
or the HTML of the page. An automated tool like this can easily fetch 100
pages and find the three pages that have errors on them. You may choose to fix
the code or just approve the answer that you just got as the new right answer.

Selenium can also be used for animating the Java Script controls for the front
end of an application. Many apps are becoming increasingly complex on the
front end and Selenium provides tools for testing out the interactions of
controls with heavy Java Script functionality. If you are going to do a lot of
this type of work you might want to investigate other tools that specialize in
this area.

User stories define the specific scenarios that users are trying to accomplish
when they use your system. Consider using Selenium as the basis of a test app
that will walk through each of the scenarios. The app can fetch a page, login,
click on certain controls, view tables, and other page content.

Example Test Scenario:

* Go to home page
* Login as "Bill Gates"
* View product selections
* Click on order product
* Fill out shipping form
* Submit order
* View "Thank You” page

Overall, Selenium is a great tool to have in your arsenal. It is worth investing
the time to learn how to make it useful in your context. If you do a lot of
web development then Selenium can help you automate most of your manual testing.


## Test Suites

Most testing advice focuses on tactics and there is quite a lot written on the
mechanics of testing. My goal is to focus on the strategic nature of testing
and how it can help you reuse software more effectively. I recommend that you
get familiar with tactical testing techniques as well. A great place to start
is with "Test-Driven Development" by Kent Beck, this is a classic that every
engineer should be familiar with.

Testing strategies are the most important tool that you have when working with
legacy code, yet they are often overlooked. Engineers often spend too much
time thinking about the code changes that are needed and not enough time
thinking about how the changes will add risk to the system. Build test suites
that implement the different strategic goals you have for testing.


### Testing Strategy

All test code should begin by capturing the exact behavior of the current
system. Implement some code to record the functional surface area of the app.
Generate a series of requests to your system and collect the various
responses. Think about hundreds of request/response pairs.

A single test case is nothing more than a request and its corresponding
response. How these test cases are executed will be a function of your system
architecture. There are numerous ways that you could implement these
interactions:

* Function call that returns a value
* Method call on an API
* HTTP request and response
* COM or Corba invocation
* Web service query
* Database query
* Key/value pairs
* Script invocation with an output

Decide on what makes sense for your system. Do the thing that is easiest and
most natural for you. You will rest a lot easier if you know that you have a
library of hundreds of test cases that you can run in a few seconds. Consider
building unique test suites for each type of interaction with your system.
This has the advantage of utilizing your system in many different ways.

For example, you may wish to have the following test suites:

* database - execute stacks of SQL commands
* web-requests - deal with HTTP requests and responses
* import-export - consume and produce text files
* key-values - verify interactions with Key/Value store

By having different suites you can make them very simple and yet flexible.
Each system invocation can use the same helper functions to call the
appropriate entry point and process the output data in a consistent way. This
scales extremely well to large system sizes.

Build simple test cases, rather than complex ones. Don't build test cases with
multiple clauses that are trying to check for multiple things at once.
Instead, build many different test cases and set these up as tables. Have a
tool that will execute everything at once so you can run all of your tests at
least once every hour. Run extended tests at least daily to ensure that you
didn't miss anything.

Think about the types of testing that are needed to provide you with
confidence that everything is working as it should be. Budget your testing
effort to focus on the most critical areas. Give special priority to:

* Areas that need a lot of new functionality
* Performance bottlenecks
* Parts with a lot of issues historically
* Sources of current defects and other issues
* Costly parts requiring extra development time
* Areas of disagreements within the team
* Modules with many changes or multiple authors

Consider creating a test plan that has a table of all modules and your key
attributes for making decisions about the priority of each. Create a scoring
scheme that matches the business and development objectives of your
organization. Score every module and use the results to set the development
priorities. Select tools and techniques for each module that will be used to
encode and execute the test cases. Now determine how many test cases you need
to feel comfortable with that module.

Testing becomes strategic when it drives the rest of the development.
Strategic testing is truly automated when it executes stacks of test cases
without intervention. A table of results tells you which tests fail so that
all failures can be fixed in seconds, not hours. Great testing should reveal
local hot spots in the code.

Solid testing makes the code healthier over time by allowing constant
refactoring - and refactoring can't be done safely without tests. Testing
provides a strong indirect benefit to the long-term health of the entire
system by turning back the effects of bit rot.


### Test Execution

All of the tests should run on a regular basis. If tests are only run on
special occasions you will have to wait a long time to find out that the
system is broken. It would be far better to find out within minutes of the
system breakdown - shorten this delay to reinforce the correct developer
behavior.

Tie the test execution into the commit process - continuous integration tools,
such as Tox and Travis, can make this easy for you. Build and test the code
each time you accept a commit to confirm that the code works properly across
the full range of execution contexts.

Additional rounds of testing should occur automatically to ensure that the
entire system remains in good shape. I recommend setting up multiple test
frequencies: hourly, daily, weekly, monthly. Hourly tests should be limited in
their scope and execute extremely fast because this is the basis of the
developer tests and will frequently have a human waiting on the test results.

The monthly test, on the other hand, may take many hours to run. No one will
be waiting for the results, so it doesn't matter how long it takes to
complete. It may provision servers, restore databases, analyze transactions,
and transform key data. In the limit, these "tests" may not be tests at all
but key elements of your operations task automation. Define the different
levels of testing so that they compliment the desired operations design.

A test suite defines a collection of test cases that execute together. Clearly
designate which suites need to run in what frequency. Then match the speed
requirements of the test suite to how frequently they are run. This makes it
convenient to run a full set of tests in a few seconds while still ensuring
that the more thorough tests are being run.


### Working with Other Test Frameworks

Like everything else in software development, testing has become increasingly
heterogeneous. Different testing frameworks are optimized for different goals.
It is often beneficial to use multiple frameworks together but this requires a
clear strategy for how to manage the test code.

The most widely used testing tools are built on the unit testing frameworks
that descend from JUnit. Collectively, the unit testing frameworks are usually
referred to as XUnit.  Diff tests can work well with the tests that have
already been developed using XUnit.

XUnit tests are built in layers: module, class, method, assertion. Each level
of abstraction provides containment for lower level constructs. The framework
will automatically find and execute all tests and then present the results in
a convenient way.

XUnit requires that all logic that you want to check be coded into the
assertions of each unit test. In other words, you must decide what to check
for as a result of every test and create an assertion that validates it. In
contrast, Diff testing will execute any test and present you with the actual
results. This allows you to approve the results after the test is written
rather than before. More importantly test maintenance is done by verifying
that the output is acceptable, not by figuring out what the output should have
been. I find that this is a significant benefit of diff testing over XUnit
Testing.

It is many times faster to fix a diff test than an XUnit test. The burden of
maintaining an array of a thousand unit tests often gets so big that the
entire idea of unit testing is abandoned halfway through the project. This
leaves the project incredibly vulnerable - it would be better to have never
attempted unit testing.

Diff testing is a welcome solution to any team that has struggled to
successfully implement unit testing. It is easy to create new tests and quick
to repair them. The typical programming session begins with lighting up the
tests to see what changed. Ten tests fail and two minutes later nine of these
are fixed and the tenth issue is something that needs attention. This case
demonstrates the benefit of this testing style.

Traditional techniques wouldn't even find these unexpected changes because
they aren't really errors at all. They are unexpected results that you should
think about. Over the course of a year this system could save hundreds of
hours of effort by giving you early warning of dangers.

The easiest way to utilize existing tests together with diff testing is to
wrap the top-level test runner as a single diff test. I use a test runner
called Nose to automatically run all my Python unit tests. Here is a diff test
that will run all of the unit tests.

    def nose_tester():
        system('nosetests -v')

The output is automatically captured from Nose. It is compared to the last
time the function was run. The "-v" option lists all of the individual tests.
The names of all the tests run are confirmed. If anyone has disabled or added
any tests the nose_tester will fail. With one line of code you can test the
tester.

In practice, most tests invoke an external shell script rather than import
code directly. This has the benefit of isolating the test invocation from the
function under test. You will need to make a decision about whether you have a
long running program with a test API or whether you are running different
processes with each test function. Pick the architecture that works for your
system but design it early on.

The most beneficial kind of testing is looking at the system context. This is
often overlooked in favor of business logic. The implications of running out
of disk space, and improper configuration of variables and paths, can be
profound. Consider the following tests and ask yourself when they might have
saved you.

    def context_test():
        bigger_than (shell ('df'), 100000)
        line_limit (shell ('find'), 0, 20000)
        system ('du  thisdir')
        system ('echo $X')
        system ('ls')
        system ('pwd')

No matter how you do testing, it is a good idea to measure the test coverage.
There are some excellent tools available for this. Search 'Test Coverage'
with your language on Google. Push for high test coverage when you run your
automated tests but accept the reality that you will not get to 100%. Use
coverage information in addition to issues reported to determine where the
hot spots of low quality are in your code. Analyze and fix areas of weak
testing as you go.


### Legacy Code

Many of the books that discuss testing focus on building new systems from
scratch. These are techniques that can be applied from the start of a project
that will put the project on stable footing. If you have the luxury of setting
the proper process in place at the beginning you will save yourself a lot of
work later.

In the real world, most projects don't have that luxury. You will often
inherit a mess that someone else created and you have to bring it under
control. Let's look at testing techniques that are particularly good for
dealing with legacy code.

A legacy system is any project that is currently deployed. It comes with a
fully completed product definition, design, and implementation. It also comes
with a history of trade-offs that were often made by a different team than the
one that has inherited it.

Although the system is complete in one sense, it is also quite incomplete. It
has issues associated with every phase of development, including the product
definition, design, and implementation. Common testing techniques often don't
work well with legacy code since they assume that you are starting a fresh
application.

The first goal of working with a legacy system is to draw a functional and
quality baseline. Evaluate the issues of each part of the life cycle to
understand how the system works currently. This will help us determine the
work priorities.

Once we have a baseline and priorities we will need to bring the system under
control. Build some scaffolding logic and instrumentation around the areas of
the code that need work. This will protect us from making errors as we begin
to extend and improve the system. Don't skip this step in the process! Jumping
directly into modifying code without a safety net will cause your software
schedule and cost to balloon.

Our goal is to leverage the testing effort between projects. We intend to
bring the full weight of understanding to bear on the new testing. Use product
functionality that remains unchanged to identify the areas where the testing
can also be recycled.

Identifying key interfaces within the system and create test solutions to
validate them. Use playback and capture techniques to thoroughly test these
interfaces. Analyze the API language of each interface in order to create a
complete test suite. These interface tests are a key point of leverage between
projects. Most interfaces remain remarkably stable over the years. Build
sophisticated methods for testing scenarios on your most critical interfaces.


## Best Practice #7 - _Use diff testing to generate maximum test coverage._



***Problem***<br>
Many testing techniques require that tests are built in parallel with the
code. Unit testing frameworks are often used to allow the developers to build
tests for each function that they write. This can be a time consuming and
tedious task. The developer's commitment to unit testing often wavers as the
project progresses. Late in the project (when it is most needed) unit testing
is often abandoned altogether.

The problem is that the testing techniques take too much time and energy to
use effectively. Legacy systems also present a unique problem. The investment
to retrofit testing onto an existing code base is enormous. This leads
commonly to very limited use of Test-Driven Development techniques in legacy
systems.

***Solution***<br>
Create some methods that utilize the ideas from XUnit but add tools that make
it easier to write and maintain tests. By making every test emit output we 
can use **diff** to verify the output. Test cases can be reduced to single 
lines of code that simply invoke some function in our system.

Instead of having to anticipate and code for the correct test answer, each
test can simply be written to produce an answer. On the next execution the
test is required to yield the same answer. With this method, tests start out
very noisy with lots of false positives, but can be easily silenced by using
filters. The maintenance burden is then drastically reduced compared to other
types of testing available.

***Next Steps***

* Create some simple tests 
* Make a list of 100 assertions that you would like to test
* Plan how you might do live data testing
* Learn about Selenium for testing your web server

